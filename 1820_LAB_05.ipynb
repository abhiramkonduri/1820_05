{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3kjok5zHBWc6CGHH+NtzK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2203a51724/IRS-2025/blob/main/irs_5_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqpkco30Nq0a",
        "outputId": "6404d4f8-e663-4ad6-eb5c-e4444e37b2b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Download necessary datasets\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = [\"running\", \"happier\", \"flies\", \"unbelievable\", \"replaying\"]\n",
        "\n",
        "for word in words:\n",
        "    lemma = lemmatizer.lemmatize(word, pos='v')  # 'v' for verbs\n",
        "    print(f\"Word: {word} -> Lemma: {lemma}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-N7yKOiN-8B",
        "outputId": "9ca5c3c8-0f7b-4204-8327-0ad860dc8481"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: running -> Lemma: run\n",
            "Word: happier -> Lemma: happier\n",
            "Word: flies -> Lemma: fly\n",
            "Word: unbelievable -> Lemma: unbelievable\n",
            "Word: replaying -> Lemma: replay\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "prefixes = [\"un\", \"re\", \"pre\", \"mis\", \"dis\", \"non\", \"in\", \"im\", \"ir\", \"il\"]\n",
        "suffixes = [\"ing\", \"ed\", \"s\", \"es\", \"er\", \"est\", \"ly\", \"ment\", \"ness\", \"able\"]\n",
        "\n",
        "def analyze_morphology(word):\n",
        "    root = word\n",
        "    prefix_found, suffix_found = \"\", \"\"\n",
        "\n",
        "    # Check for prefixes\n",
        "    for prefix in prefixes:\n",
        "        if word.startswith(prefix):\n",
        "            prefix_found = prefix\n",
        "            root = word[len(prefix):]\n",
        "            break\n",
        "\n",
        "    # Check for suffixes\n",
        "    for suffix in suffixes:\n",
        "        if word.endswith(suffix):\n",
        "            suffix_found = suffix\n",
        "            root = root[:-len(suffix)]\n",
        "            break\n",
        "\n",
        "    return {\"Word\": word, \"Prefix\": prefix_found, \"Root\": root, \"Suffix\": suffix_found}\n",
        "\n",
        "words = [\"unbelievable\", \"replaying\", \"happiness\", \"disapprove\", \"running\"]\n",
        "\n",
        "for word in words:\n",
        "    print(analyze_morphology(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCc2Pi3-OAGU",
        "outputId": "369824b4-f2f7-4223-bd4d-c990432dd582"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Word': 'unbelievable', 'Prefix': 'un', 'Root': 'believ', 'Suffix': 'able'}\n",
            "{'Word': 'replaying', 'Prefix': 're', 'Root': 'play', 'Suffix': 'ing'}\n",
            "{'Word': 'happiness', 'Prefix': '', 'Root': 'happines', 'Suffix': 's'}\n",
            "{'Word': 'disapprove', 'Prefix': 'dis', 'Root': 'approve', 'Suffix': ''}\n",
            "{'Word': 'running', 'Prefix': '', 'Root': 'runn', 'Suffix': 'ing'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "word = \"replaying\"\n",
        "doc = nlp(word)\n",
        "\n",
        "for token in doc:\n",
        "    print(f\"Word: {token.text}\")\n",
        "    print(f\"Lemma: {token.lemma_}\")\n",
        "    print(f\"Morphology: {token.morph}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hfWTw0HOGkp",
        "outputId": "e8440446-221c-4038-f752-339f8860470d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: replaying\n",
            "Lemma: replay\n",
            "Morphology: Aspect=Prog|Tense=Pres|VerbForm=Part\n"
          ]
        }
      ]
    }
  ]
}